# lianjia_spider
链家深圳租房爬虫

使用python3.6 + scrapy + fake_useragent

爬取了深圳所有租房的信息，一共2.8w+条
该爬虫可拓展多个城市，只要修改一下起始url中的sz改为对应的城市拼音首字母即可

## 问题
链家web端最多爬取100页信息，每页30条，如果直接爬取最多抓取3000条信息。
## 解决
分区域（大区还是超过3000条），再细分到商圈（上百个商圈，商圈最多的也就千条左右），计算页码，然后爬取。

## 更好的方法
看公众号文章发现别人用api提取json的，使用fiddler以后m端和app端确实都有api。。。。。提取json大大快与解析网页。
